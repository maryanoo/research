{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39794cdc-fc38-4623-bda5-cf4310d33aa8",
   "metadata": {},
   "source": [
    "System zarządzania literaturą naukową. Ma on charakter kołowy, pobieraj, analizuj, pobieraj. \n",
    "Zaczynamy od pobierania literatury.\n",
    "Do reasearch warto użyć przeglądarkę firefox + Break Down the Walls. Ustawienie Anna's Library.\n",
    "Następnie wyciągnięcie zawartości oraz bibliografi (działa lepiej dla artykułów, niż książek)\n",
    "Strona http://cermine.ceon.pl/index.html - pojedyncze pozycje\n",
    "Lub aplikacja https://github.com/CeON/CERMINE - odpalana z lini komend, lub jako masowa obróbka plików. \n",
    "Ostatnia wersja programu z 2017 roku.\n",
    "$ java -cp cermine-impl-1.13-jar-with-dependencies.jar pl.edu.icm.cermine.ContentExtractor -output jats -path path/to/directory/with/pdfs/ \n",
    "Bibtex bezpośrednio nie działa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b0b09-5be4-498f-97ab-2dbeafce9166",
   "metadata": {},
   "outputs": [],
   "source": [
    "#skrypt do automatycznego wyciągania informacji blibliograficznejz plików pdf.\n",
    "#!/bin/bash\n",
    "\n",
    "# Ścieżka do programu java\n",
    "JAVA_PATH=\"/usr/bin/java\"\n",
    "\n",
    "# Ścieżka do jar zależności\n",
    "CERMINE_JAR_PATH=\"/home/mariusz/cermine-impl-1.13-jar-with-dependencies.jar\"\n",
    "\n",
    "# Ścieżka głównego katalogu do przeszukania\n",
    "MAIN_DIR=\"/mnt/backupall/ksiazki_1/\"\n",
    "\n",
    "# Funkcja do sprawdzania czy plik jest mniejszy niż 10MB\n",
    "function is_smaller_than_10mb {\n",
    "    local file_size=$(stat -c %s \"$1\")\n",
    "    if [[ $file_size -lt 10000000 ]]; then\n",
    "        return 0\n",
    "    else\n",
    "        return 1\n",
    "    fi\n",
    "}\n",
    "\n",
    "# Funkcja do przetwarzania plików PDF\n",
    "function process_pdf {\n",
    "    local pdf_dir=$(dirname \"$1\")\n",
    "    echo \"Przetwarzanie katalogu: $pdf_dir\"\n",
    "    $JAVA_PATH -cp $CERMINE_JAR_PATH pl.edu.icm.cermine.ContentExtractor -outputs jats -path \"$pdf_dir\"\n",
    "}\n",
    "\n",
    "# Główna pętla\n",
    "while true; do\n",
    "    # Znajdź wszystkie pliki PDF w podkatalogach\n",
    "    find \"$MAIN_DIR\" -type f -name \"*.pdf\" | while read -r pdf_file; do\n",
    "        if is_smaller_than_10mb \"$pdf_file\"; then\n",
    "            process_pdf \"$pdf_file\"\n",
    "        else\n",
    "            echo \"Plik $pdf_file jest większy niż 10MB, pomijanie...\"\n",
    "        fi\n",
    "    done\n",
    "    \n",
    "    # Sprawdź, czy są jeszcze jakieś pliki do przetworzenia\n",
    "    if [[ $(find \"$MAIN_DIR\" -type f -name \"*.pdf\" | wc -l) -eq 0 ]]; then\n",
    "        echo \"Brak plików PDF do przetworzenia, zakończono.\"\n",
    "        break\n",
    "    else\n",
    "        echo \"Znaleziono więcej plików PDF do przetworzenia, kontynuowanie...\"\n",
    "    fi\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a592c933-cb79-463c-97a3-323ed63a943a",
   "metadata": {},
   "source": [
    "Efektem działania jest katalog z plikami .cermxml Mają one strukturę xml dla literatury naukowej https://jbats.nlm.nih.gov/taglib.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ed70a8-72dc-4472-ab2b-49fd781368e0",
   "metadata": {},
   "source": [
    "Kolejny krok to wyciągnięcie danych z plików i zapis ich w pliku csv. Kod w pythonie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1387ff09-5733-448c-bf65-d61a06725948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xmltodict\n",
    "import csv\n",
    "\n",
    "# Katalog z plikami CERMXML\n",
    "directory = \"crxml\"\n",
    "\n",
    "# Ścieżka do pliku CSV\n",
    "csv_file = \"bibliography_xml.csv\"\n",
    "\n",
    "# Otwieramy plik CSV w trybie zapisu\n",
    "with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    # Definiujemy nagłówki kolumn\n",
    "    fieldnames = [\"Authors\", \"Title\", \"Year\", \"Journal\", \"DOI\"]\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    \n",
    "    # Zapisujemy nagłówki do pliku CSV\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Pętla po plikach w katalogu\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".cermxml\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            try:\n",
    "                # Wczytanie pliku CERMXML\n",
    "                with open(filepath, \"r\") as xml_file:\n",
    "                    xml_data = xml_file.read()\n",
    "\n",
    "                # Konwersja XML do słownika\n",
    "                data = xmltodict.parse(xml_data)\n",
    "\n",
    "                # Sprawdzenie czy istnieje sekcja ref-list\n",
    "                if \"article\" in data and \"back\" in data[\"article\"] and \"ref-list\" in data[\"article\"][\"back\"]:\n",
    "                    # Przetwarzanie tylko elementów ref\n",
    "                    for reference in data[\"article\"][\"back\"][\"ref-list\"].get(\"ref\", []):\n",
    "                        # Sprawdzenie czy element mixed-citation istnieje\n",
    "                        mixed_citation = reference.get(\"mixed-citation\", {})\n",
    "                        if isinstance(mixed_citation, dict):\n",
    "                            # Pobranie autorów\n",
    "                            authors = []\n",
    "                            author_data = mixed_citation.get(\"string-name\", [])\n",
    "                            if isinstance(author_data, list):\n",
    "                                for name in author_data:\n",
    "                                    surname = name.get(\"surname\", \"\")\n",
    "                                    given_names = name.get(\"given-names\", \"\")\n",
    "                                    full_name = f\"{given_names} {surname}\"\n",
    "                                    authors.append(full_name)\n",
    "                            # Pobranie tytułu\n",
    "                            title = mixed_citation.get(\"article-title\", \"\")\n",
    "                            # Pobranie roku wydania\n",
    "                            year = mixed_citation.get(\"year\", \"\")\n",
    "                            # Pobranie czasopisma\n",
    "                            journal = mixed_citation.get(\"source\", \"\")\n",
    "                            # Pobranie DOI\n",
    "                            doi = mixed_citation.get(\"xref\", \"\")\n",
    "\n",
    "                            # Zapisujemy dane do pliku CSV\n",
    "                            writer.writerow({\n",
    "                                \"Authors\": \"; \".join(authors),\n",
    "                                \"Title\": title,\n",
    "                                \"Year\": year,\n",
    "                                \"Journal\": journal,\n",
    "                                \"DOI\": doi\n",
    "                            })\n",
    "\n",
    "            except Exception as e:\n",
    "                # Obsługa błędów: zapisz informacje o błędzie do pliku dziennika\n",
    "                with open(\"error_log.txt\", \"a\") as log_file:\n",
    "                    log_file.write(f\"Error processing file {filepath}: {str(e)}\\n\")\n",
    "                continue  # Kontynuuj przetwarzanie kolejnych plików"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eddbb94-0627-41b2-94ea-6ea1e6158ce4",
   "metadata": {},
   "source": [
    "Następnie można zrobić Name Entity Recognition (NER) aby rozpoznać nazwiska z kolumy Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43493475-ec06-425a-8dd8-6a152a4016fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "from langdetect import detect\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ścieżka do folderu zawierającego pliki cermxml\n",
    "cermxml_folder = \"crxml\"\n",
    "\n",
    "# Funkcja do przeprowadzenia detekcji NER dla danego pliku\n",
    "def detect_ner(file_path, language):\n",
    "    if language == \"pl\":\n",
    "        model_name = \"pl_core_news_lg\"\n",
    "    elif language == \"en\":\n",
    "        model_name = \"en_core_web_md\"\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported language\")\n",
    "    nlp = spacy.load(model_name)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "        doc = nlp(text)\n",
    "        entities = [ent.text for ent in doc.ents if ent.label_ in ['PERSON', 'ORG', 'GPE']]\n",
    "    return entities\n",
    "\n",
    "# Lista przechowująca wyniki detekcji\n",
    "results = []\n",
    "skipped_files = []\n",
    "\n",
    "# Przeprowadzenie detekcji NER dla każdego pliku\n",
    "files = [file for file in os.listdir(cermxml_folder) if file.endswith(\".cermxml\")]\n",
    "for file in tqdm(files, desc=\"Processing files\", unit=\"file\"):\n",
    "    file_path = os.path.join(cermxml_folder, file)\n",
    "    # Wykrycie języka dla danego pliku\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "        try:\n",
    "            language = detect(text)\n",
    "        except:\n",
    "            language = \"unknown\"\n",
    "    try:\n",
    "        entities = detect_ner(file_path, language)\n",
    "        results.append({\"source\": file, \"entities\": entities})\n",
    "    except ValueError as e:\n",
    "        skipped_files.append(file)\n",
    "        print(f\"Skipping file {file}: {e}\")\n",
    "\n",
    "# Utworzenie ramki danych\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Zapisanie ramki danych do pliku CSV\n",
    "df.to_csv(\"cermxml-NER.csv\", index=False)\n",
    "\n",
    "# Zapisanie informacji o pominiętych plikach do pliku tekstowego\n",
    "with open(\"skipped_files.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(skipped_files))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60996a07-68c4-4dd8-8a87-0bdb01febaa3",
   "metadata": {},
   "source": [
    "Czysczenie plików. Krok pierwszy - usuwanie pustych linijek. Pliki cermxml czasem nie są właściwie ustrukturyzowane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6c4c33-84ec-48c9-8f0a-6fe45d1452d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Wczytaj plik CSV\n",
    "df = pd.read_csv(\"cermxml-NER.csv\")\n",
    "\n",
    "# Usuń puste wiersze\n",
    "df = df.dropna()\n",
    "\n",
    "# Funkcja do przekształcenia listy encji w czytelną postać\n",
    "def format_entities(entities):\n",
    "    # Łączy elementy listy w pojedynczy ciąg znaków, oddzielając je przecinkiem\n",
    "    return ', '.join(entities)\n",
    "\n",
    "# Przekształć listy encji w czytelną postać\n",
    "df['entities'] = df['entities'].apply(eval).apply(format_entities)\n",
    "\n",
    "# Zapisz oczyszczoną ramkę danych do pliku CSV\n",
    "df.to_csv(\"cermxml-NER-cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93e9758-ecf3-4430-b029-539556c3564e",
   "metadata": {},
   "source": [
    "Usuwanie znaczników xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad8d335-9f5e-4127-9505-af344c5d0ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Wczytaj plik CSV\n",
    "df = pd.read_csv(\"cermxml-NER-cleaned.csv\")\n",
    "\n",
    "# Usuń puste wiersze\n",
    "df = df.dropna()\n",
    "\n",
    "# Funkcja do parsowania listy encji\n",
    "def parse_entities(entities_str):\n",
    "    # Podziel listę encji na pojedyncze elementy\n",
    "    entities_list = entities_str.strip(\"[]\").split(\", \")\n",
    "    # Usuń cudzysłowy i znaki specjalne z każdego elementu listy\n",
    "    cleaned_entities = [entity.strip(\"''\").replace(\"¨\", \"\") for entity in entities_list]\n",
    "    return cleaned_entities\n",
    "\n",
    "# Parsuj listy encji i usuń znaczniki HTML\n",
    "df['entities'] = df['entities'].apply(parse_entities)\n",
    "df['entities'] = df['entities'].apply(lambda x: ', '.join(x))\n",
    "df['entities'] = df['entities'].apply(lambda x: BeautifulSoup(x, \"html.parser\").get_text())\n",
    "\n",
    "# Zapisz oczyszczoną ramkę danych do pliku CSV\n",
    "df.to_csv(\"cermxml-NER-cleaned-html.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e228b9cb-89cf-4711-8986-55560b293ca8",
   "metadata": {},
   "source": [
    "#Kolejna część to analiza tematów. Klasyfikacja odbywa się przy użyciu BERTa i SpacY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce416df2-14de-4c43-a5f1-342b69f6ca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    # Wczytaj plik CSV\n",
    "    df = pd.read_csv(\"bibliography_xml.csv\")\n",
    "\n",
    "    # Usuń puste wartości w kolumnie 'Title'\n",
    "    df = df.dropna(subset=['Title'])\n",
    "\n",
    "    # Wybierz tylko kolumnę 'Title'\n",
    "    titles = df['Title'].tolist()\n",
    "\n",
    "    # Inicjalizuj BERTopic\n",
    "    topic_model = BERTopic()\n",
    "\n",
    "    # Dopasuj model do danych\n",
    "    try:\n",
    "        with tqdm(total=len(titles), desc=\"Fitting BERTopic\") as pbar:\n",
    "            topic_model.fit(titles)\n",
    "            pbar.update(1)\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred during model fitting:\", e)\n",
    "\n",
    "    # Przypisz etykiety tematów z powrotem do ramki danych\n",
    "    try:\n",
    "        topic_labels, _ = topic_model.transform(titles)\n",
    "        df['Topic'] = topic_labels\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred while assigning topic labels:\", e)\n",
    "\n",
    "    # Mapuj identyfikatory tematów na ich nazwy\n",
    "    topic_freq = topic_model.get_topic_freq()\n",
    "    topic_name_map = {topic[0]: f\"Topic {topic[0]}\" for topic in topic_freq}\n",
    "\n",
    "    # Dodaj kolumnę 'Topic Name' z nazwami tematów\n",
    "    df['Topic Name'] = df['Topic'].map(topic_name_map)\n",
    "\n",
    "    # Zapisz wyniki do pliku CSV\n",
    "    df[['Title', 'Topic', 'Topic Name']].to_csv(\"wyniki_bertopic.csv\", index=False)\n",
    "\n",
    "    # Wyświetl tylko 10 pierwszych wierszy\n",
    "    print(df[['Title', 'Topic', 'Topic Name']].head(10))\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da1f742-207d-4371-b760-59bb1d2f43c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pobierz słownik etykiet tematów i odpowiadających im słów kluczowych\n",
    "topics = topic_model.get_topics()\n",
    "\n",
    "# Przejrzyj słownik etykiet tematów i przypisanych im słów kluczowych\n",
    "for topic, keywords in topics.items():\n",
    "    print(f\"Topic {topic}: {keywords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adad3a89-03e7-4451-a682-403f652619b5",
   "metadata": {},
   "source": [
    "Stworzenie listy najpopilarniejszych słów (rzeczowników i czasowników) w formach podstawowych. Kod optymalizowany, żeby działał na wielu rdzeniach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057e1eea-083b-4c17-b915-4cffe095d539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Load the Polish and English language models\n",
    "nlp_pl = spacy.load(\"pl_core_news_lg\")\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a function to clean XML tags from text\n",
    "def clean_xml_tags(text):\n",
    "    soup = BeautifulSoup(text, \"xml\")\n",
    "    return soup.get_text()\n",
    "\n",
    "# Define a function to extract lemmas from text\n",
    "def extract_lemmas(text, language):\n",
    "    if language == 'pl':\n",
    "        nlp = nlp_pl\n",
    "    elif language == 'en':\n",
    "        nlp = nlp_en\n",
    "    else:\n",
    "        return None\n",
    "    # Tokenize the text\n",
    "    doc = nlp(text)\n",
    "    # Initialize counters for verbs and nouns\n",
    "    verb_counter = Counter()\n",
    "    noun_counter = Counter()\n",
    "    # Count verbs and nouns\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'VERB':\n",
    "            verb_counter[token.lemma_] += 1\n",
    "        elif token.pos_ == 'NOUN':\n",
    "            noun_counter[token.lemma_] += 1\n",
    "    # Filter lemmas to include only verbs and nouns occurring more than 30 times\n",
    "    filtered_lemmas = {lemma: count for lemma, count in verb_counter.items() if count > 15}\n",
    "    filtered_lemmas.update({lemma: count for lemma, count in noun_counter.items() if count > 15})\n",
    "    return filtered_lemmas\n",
    "\n",
    "# Define a function to process a single file\n",
    "def process_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        # Clean XML tags\n",
    "        cleaned_text = clean_xml_tags(text)\n",
    "        try:\n",
    "            language = detect(cleaned_text)\n",
    "            lemmas = extract_lemmas(cleaned_text, language)\n",
    "            if lemmas:\n",
    "                return lemmas\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "# Define the directory containing the XML files\n",
    "directory = 'crxml'\n",
    "\n",
    "# Initialize list to store results\n",
    "results = []\n",
    "\n",
    "# Process each file in the directory using concurrent processing\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.cermxml'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            future = executor.submit(process_file, file_path)\n",
    "            futures.append((filename, future))\n",
    "\n",
    "    # Retrieve results from futures\n",
    "    for filename, future in tqdm(futures):\n",
    "        lemmas = future.result()\n",
    "        if lemmas:\n",
    "            for lemma, count in lemmas.items():\n",
    "                results.append((filename, lemma, count))\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "df = pd.DataFrame(results, columns=[\"nazwa_pliku\", \"lemma\", \"liczba\"])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"wyniki_analizy-opt.csv\", index=False)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
